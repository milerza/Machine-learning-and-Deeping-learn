# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xUjCv3DaMUCoVaFIQjQsOINwp1cwwpvp
"""

import numpy #tratamento de matrizes
from keras.models import Sequential #criar modelos
from keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization, Activation #layers aplicados para definir o modelo
from keras.constraints import maxnorm #funcoes de resticao
from keras.layers.convolutional import Conv2D, MaxPooling2D #funcoes de convulucao
from keras.utils import np_utils #utilitarios para manipulacao de listas 
from keras.datasets import cifar10 #dataset com dados para treinamento

seed = 30 #vezes que replico a "seed"
numpy.random.seed(seed) #colocar aleatorio a "seed" para reproduzir

(X_train, y_train),(X_test, y_test) = cifar10.load_data() #carregando os dados(X_entrada,y_saida)

#simplificar as entradas (normalize) os dados de 0-255 para 0-1
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train = X_train / 255.0
X_test = X_test / 255.0

#configurando a saida ---> converte classe inteira para binaria

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1] # shape retorna (n,m) da matriz ---> retornando o numero m de colunas

#criar modelo
model = Sequential()

#32 filtros
#3x3 o tamanho do filtro
#forma de entrada = primeira cada
#tamanho da imagem n alterado
model.add(Conv2D(32,(3,3),input_shape=X_train.shape[1:], padding='same'))

#funcao de "pesos" para as entradas
model.add(Activation('relu'))

#desfazendo algumas conexoes para evitar overfitting
model.add(Dropout(0.2))

#normalizar o lote apos a funcao de ativacao ser executada e algumas ligacoes forem cortadas
#normalizar os dados novamente para que na prox etapa estejam organizados de 0-1
model.add(BatchNormalization())

model.add(Conv2D(32, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2))) #Poolling -> reduz as matrizes para pegar apenas dados relevantes, evita overfitting
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Conv2D(128, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Flatten())#achatar os dados para processamento
model.add(Dropout(0.2))

#camada densamente conectada
model.add(Dense(256, kernel_constraint=maxnorm(3)))#coloca restricao para os parametos dos pesos 
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(Dense(128, kernel_constraint=maxnorm(3)))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())
model.add(Dense(num_classes))
model.add(Activation('softmax'))#diferente da relu q multiplica por pesos, a softmaax seleciona os melhores neuronios

epocas = 30
otimizador = 'adam'

#compilar modelo
model.compile(loss='categorical_crossentropy', optimizer=otimizador, metrics=['accuracy'])
print(model.summary())

#treinar modelo

model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epocas, batch_size=64)

acertos = model.evaluate(X_test, y_test, verbose=0)
print("Accuracy: %.2f%%" % (acertos[1]*100))